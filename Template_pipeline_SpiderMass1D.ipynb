{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbc9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kruskal\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def import_data(data_file):\n",
    "    \"\"\"\n",
    "    Imports data from a CSV file and performs initial data preprocessing.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_file)\n",
    "    data = data.drop([\"File\", \"Start scan\", \"End scan\", \"Sum.\"], axis=1)\n",
    "    return data\n",
    "\n",
    "def load_and_process_data(data_file):\n",
    "    # Load the dataset\n",
    "    train = pd.read_csv(data_file)\n",
    "\n",
    "    # Data preprocessing\n",
    "    train_id = train\n",
    "    train = train.drop([\"File\", \"Sum.\", \"Start scan\", \"End scan\"], axis=1) # adapt this to your specific columns\n",
    "\n",
    "    # Split data into features (X) and target (y)\n",
    "    y = train.pop('Class')\n",
    "    X = train\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def lazy_predict(X_train, y_train, X_test, y_test):\n",
    "    # Initialize LazyClassifier\n",
    "    clf = LazyClassifier(verbose=0, predictions=True)\n",
    "    models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Print the models\n",
    "    print(models)\n",
    "\n",
    "    return models, predictions\n",
    "\n",
    "def find_and_build_best_model(models, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Find and build the best model based on Lazy Predict results.\n",
    "    \"\"\"\n",
    "    best_model_name = None\n",
    "    best_f1_score = -1\n",
    "\n",
    "    for model_name in models.index:\n",
    "        f1_score = models.at[model_name, 'F1 Score']\n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "            best_model_name = model_name\n",
    "\n",
    "    if best_model_name:\n",
    "        print(\"Best Classifier:\", best_model_name)\n",
    "\n",
    "        try:\n",
    "            model_module = importlib.import_module('sklearn.linear_model')\n",
    "            if hasattr(model_module, best_model_name):\n",
    "                best_model = getattr(model_module, best_model_name)()\n",
    "            else:\n",
    "                model_module = importlib.import_module('sklearn.ensemble')\n",
    "                if hasattr(model_module, best_model_name):\n",
    "                    best_model = getattr(model_module, best_model_name)()\n",
    "                else:\n",
    "                    # Check if it's a LightGBM classifier\n",
    "                    if best_model_name.startswith(\"LGBM\"):\n",
    "                        best_model = getattr(lgb, best_model_name)()\n",
    "                    # Check if it's an XGBoost classifier\n",
    "                    elif best_model_name.startswith(\"XGB\"):\n",
    "                        best_model = getattr(xgb, best_model_name)()\n",
    "                    else:\n",
    "                        print(\"Best Classifier not found.\")\n",
    "                        return None, None\n",
    "            \n",
    "            pipeline = Pipeline([('scaler', StandardScaler()), (best_model_name, best_model)])\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            return best_model_name, pipeline\n",
    "        except ImportError:\n",
    "            print(\"Best Classifier not found.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Best Classifier not found.\")\n",
    "        return None, None\n",
    "\n",
    "def confusion_matrix_scores_classification_report(pipeline, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Show scores, confusion matrix, and classification report.\n",
    "    \"\"\"\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print('Accuracy:', score)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    ConfusionMatrixDisplay.from_estimator(pipeline, X_test, y_test)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 15)\n",
    "    plt.show()\n",
    "\n",
    "def cross_validate_and_report(pipeline, X, y):\n",
    "    \"\"\"\n",
    "    K-fold cross-validation and show scores, classification report, and confusion matrix.\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "    print('CV Scores:', cv_scores)\n",
    "    print('Mean CV Score:', cv_scores.mean())\n",
    "    print('Std CV Score:', cv_scores.std())\n",
    "\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=kfold)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    class_names = pipeline.named_steps[pipeline.steps[-1][0]].classes_\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='viridis')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=class_names, yticklabels=class_names, title='Confusion matrix', ylabel='True label', xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"black\" if cm[i, j] > thresh else \"yellow\")\n",
    "    fig.tight_layout()\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 15)\n",
    "    plt.show()\n",
    "\n",
    "def eli5_feature_importance(pipeline, X_train, top_features=40):\n",
    "    \"\"\"\n",
    "    Show sample contributions using Eli5 for one sample in the train set.\n",
    "    \"\"\"\n",
    "    model = pipeline.named_steps[pipeline.steps[-1][0]]\n",
    "    sample_contribution = eli5.show_weights(model, feature_names=X_train.columns.tolist(), top=top_features, feature_re='^.*$')\n",
    "    return sample_contribution\n",
    "\n",
    "def save_contributions(csv_name, pipeline, X_train):\n",
    "    \"\"\"\n",
    "    Save contributions in CSV format for all samples in the train set.\n",
    "    \"\"\"\n",
    "    model = pipeline.named_steps[pipeline.steps[-1][0]]\n",
    "    sample_contributions = []\n",
    "\n",
    "    for idx in range(len(X_train.index)):\n",
    "        sample_contribution_df = eli5.explain_weights_df(model, feature_names=X_train.columns.tolist(), feature_re='^.*$')\n",
    "        sample_contributions.append(sample_contribution_df)\n",
    "\n",
    "    all_contributions_df = pd.concat(sample_contributions)\n",
    "    all_contributions_df.to_csv(csv_name, index=False)\n",
    "    \n",
    "\n",
    "def peak_picking(ms_data, min_sn=10):\n",
    "    \"\"\"\n",
    "    Performs peak picking on mass spectrometry data using S/N ratio.\n",
    "    \"\"\"\n",
    "    peaks_df_list = []\n",
    "\n",
    "    for i in range(len(ms_data)):\n",
    "        spectrum = ms_data.iloc[i].values\n",
    "\n",
    "        # Calculate the noise level as the standard deviation of the spectrum\n",
    "        noise_std = np.std(spectrum)\n",
    "\n",
    "        # Calculate the threshold based on the noise level and min_sn\n",
    "        threshold = noise_std * min_sn\n",
    "\n",
    "        # Find peaks using the threshold\n",
    "        peaks, _ = find_peaks(spectrum, height=threshold)\n",
    "\n",
    "        peaks_df_i = pd.DataFrame({\n",
    "            'spectrum_index': i,\n",
    "            'm/z': ms_data.columns[peaks],\n",
    "            'intensity': spectrum[peaks],\n",
    "        })\n",
    "\n",
    "        peaks_df_list.append(peaks_df_i)\n",
    "\n",
    "    peaks_df = pd.concat(peaks_df_list, ignore_index=True)\n",
    "    peaks_df = peaks_df.dropna(subset=['m/z'])\n",
    "    peaks_df = peaks_df.pivot_table(index='spectrum_index', columns='m/z', values='intensity')\n",
    "    data_pick_picked = pd.concat([peaks_df,data['Class']], axis=1)\n",
    "    data_pick_picked = data_pick_picked.fillna(0)\n",
    "    \n",
    "    return data_pick_picked  \n",
    "\n",
    "def create_heatmap(data):\n",
    "    sns.clustermap(data.groupby('Class').mean().T, cmap=\"viridis_r\", center=0, col_cluster=False, row_cluster=True, metric='euclidean', z_score=0, cbar_kws={'label': ''}, cbar=True,xticklabels=True, yticklabels=False)\n",
    "    plt.show()\n",
    "\n",
    "def significant_features(data, alpha=0.05):\n",
    "    x = 'Class'\n",
    "    y_columns = data.columns.tolist()\n",
    "    \n",
    "    # Check if 'Class' is in the list of columns\n",
    "    if x in y_columns:\n",
    "        y_columns.remove(x)\n",
    "    else:\n",
    "        print(\"'Class' column not found in the data.\")\n",
    "        return None\n",
    "\n",
    "    order = data[x].unique()\n",
    "    significant_columns = []\n",
    "\n",
    "    num_comparisons = len(y_columns)  # Number of comparisons (number of features)\n",
    "\n",
    "    # Apply Bonferroni correction\n",
    "    corrected_alpha = alpha / num_comparisons\n",
    "\n",
    "    for col in y_columns:\n",
    "        data_dict = {group: data[col][data[x] == group] for group in order}\n",
    "        test_statistic, p_value = kruskal(*data_dict.values())\n",
    "        \n",
    "        if p_value <= corrected_alpha:\n",
    "            significant_columns.append(col)\n",
    "\n",
    "    return significant_columns\n",
    "\n",
    "\n",
    "def boxplot_significant_features(data, mz_values, class_colors=None, test='Kruskal'):\n",
    "    # Define order and box pairs\n",
    "    label = 'Class'\n",
    "    order = sorted(data[label].unique())  # Sort the unique class labels\n",
    "    # Generate box pairs using itertools.combinations\n",
    "    box_pairs = list(combinations(order, 2))\n",
    "    print(\"Class labels in dataset:\", order)  # Add this line for debugging\n",
    "    \n",
    "    # Create a custom color palette mapping class labels to colors\n",
    "    custom_palette = {class_label: class_colors.get(class_label, 'blue') for class_label in order}\n",
    "    \n",
    "    # Calculate the number of rows and columns for the grid layout\n",
    "    num_mz_values = len(mz_values)\n",
    "    num_cols = int(num_mz_values ** 0.5)  # Calculate the number of columns based on sqrt(num_mz_values)\n",
    "    num_rows = (num_mz_values + num_cols - 1) // num_cols  # Calculate the number of rows\n",
    "    \n",
    "    # Calculate the figure size based on the number of box plots\n",
    "    figsize_x = 16\n",
    "    figsize_y = 5 * num_rows\n",
    "    \n",
    "    # Create a figure and axis grid for the boxplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(figsize_x, figsize_y), dpi=100)\n",
    "    \n",
    "    # Flatten the axes array if there's only one row\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, mz in enumerate(mz_values):\n",
    "        x = \"Class\"\n",
    "        y = mz\n",
    "\n",
    "        # Select the current axis\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Create a boxplot on the current axis with custom class color\n",
    "        sns.boxplot(data=data, x=x, y=y, order=order, ax=ax, palette=custom_palette)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        \n",
    "        # Add simplified statistical annotations to the boxplot\n",
    "        add_stat_annotation(ax, data=data, x=x, y=y, order=order, box_pairs=box_pairs,\n",
    "                            test=test, text_format='star', loc='outside', verbose=0)\n",
    "        \n",
    "        # Set smaller font size for y-axis labels\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(num_mz_values, num_rows * num_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "    \n",
    "    # Adjust layout and spacing of subplots\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the final figure\n",
    "    plt.show()\n",
    "        \n",
    "def violin_significant_features(data, mz_values, class_colors=None, test='Kruskal'):\n",
    "    label = 'Class'\n",
    "    order = sorted(data[label].unique())  # Sort the unique class labels\n",
    "    box_pairs = list(combinations(order, 2))\n",
    "\n",
    "    print(\"Class labels in dataset:\", order)  # Add this line for debugging\n",
    "\n",
    "    # Create a custom color palette mapping class labels to colors\n",
    "    custom_palette = {class_label: class_colors.get(class_label, 'blue') for class_label in order}\n",
    "\n",
    "    # Calculate the number of rows and columns for the grid layout\n",
    "    num_mz_values = len(mz_values)\n",
    "    num_cols = int(num_mz_values ** 0.5)  # Calculate the number of columns based on sqrt(num_mz_values)\n",
    "    num_rows = (num_mz_values + num_cols - 1) // num_cols  # Calculate the number of rows\n",
    "\n",
    "    # Calculate the figure size based on the number of violin plots\n",
    "    figsize_x = 16\n",
    "    figsize_y = 5 * num_rows\n",
    "\n",
    "    # Create a figure and axis grid for the violin plots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(figsize_x, figsize_y), dpi=100)\n",
    "\n",
    "    # Flatten the axes array if there's only one row\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, mz in enumerate(mz_values):\n",
    "        x = \"Class\"\n",
    "        y = mz\n",
    "\n",
    "        # Select the current axis\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Create a violin plot on the current axis with custom class color\n",
    "        sns.violinplot(data=data, x=x, y=y, order=order, inner=\"quart\", ax=ax, palette=custom_palette)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "        # Add simplified statistical annotations to the violin plot\n",
    "        add_stat_annotation(ax, data=data, x=x, y=y, order=order, box_pairs=box_pairs,\n",
    "                            test=test, text_format='star', loc='outside', verbose=0)\n",
    "\n",
    "        # Set smaller font size for y-axis labels\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "    # Remove empty subplots\n",
    "    for i in range(num_mz_values, num_rows * num_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    # Adjust layout and spacing of subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the final figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usages\n",
    "\n",
    "# Data file path\n",
    "data_file = 'ffpe et frais neg metabolites.data.csv'\n",
    "# Import data\n",
    "data = import_data(data_file)\n",
    "\n",
    "# ML models comparison\n",
    "X, y = load_and_process_data(data_file)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)\n",
    "models, predictions = lazy_predict(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Find the best model\n",
    "best_model_name, best_model_pipeline = find_and_build_best_model(models, X_train, y_train)\n",
    "\n",
    "# Build the best model for 20% out validation \n",
    "confusion_matrix_scores_classification_report(best_model_pipeline, X_test, y_test)\n",
    "\n",
    "# Cross validate the best model with 5-fold cv\n",
    "cross_validate_and_report(best_model_pipeline, X, y)\n",
    "\n",
    "# Display feature contributions\n",
    "sample_contribution = eli5_feature_importance(best_model_pipeline, X_train)\n",
    "sample_contribution\n",
    "\n",
    "# Peak picking \n",
    "ms_data = data.drop([\"Class\"], axis=1)\n",
    "data_peak_picked = peak_picking(ms_data, min_sn=5)\n",
    "\n",
    "# Create a heatmap\n",
    "create_heatmap(data_peak_picked)\n",
    "\n",
    "# Significant features\n",
    "significant_mz_values = significant_features(data_peak_picked)\n",
    "custom_colors = {'ClassA': 'red', 'ClassB': 'green', 'ClassC': 'blue'}  # Customize class colors\n",
    "\n",
    "# Create boxplots for significant features\n",
    "boxplot_significant_features(data_peak_picked, significant_mz_values, class_colors=custom_colors)\n",
    "\n",
    "# Create violin plots for significant features\n",
    "violin_significant_features(data_peak_picked, significant_mz_values, class_colors=custom_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
