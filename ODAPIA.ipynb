{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1b2c51",
   "metadata": {},
   "source": [
    "# ODAPIA : One Dimension Analysis Pipeline \"Inteligence-Artificial\"\n",
    "version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kruskal\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import importlib\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "import eli5\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def import_data(data_file):\n",
    "    \"\"\"\n",
    "    Imports data from a CSV.\n",
    "    Args:\n",
    "        data_file (str): Path to the CSV data file.\n",
    "    Returns:\n",
    "        pd.DataFrame:data.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_file)\n",
    "    return data\n",
    "\n",
    "def plot_mean_spectra(data, class_column='Class', threshold=None, colors=None):\n",
    "    \"\"\"\n",
    "    Plots the mean spectrum for each unique target class in the specified class column.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the spectral data.\n",
    "        class_column (str): The name of the column containing class labels.\n",
    "        threshold (float): Optional threshold value for highlighting peaks.\n",
    "        colors (dict): A dictionary specifying custom colors for each class.\n",
    "\n",
    "    Returns:\n",
    "        fig (go.Figure): The Plotly figure containing the mean spectra.\n",
    "    \"\"\"\n",
    "    data = data.drop([\"Start scan\", \"End scan\", \"Sum.\", \"File\"], axis=1)\n",
    "    fig = go.Figure()\n",
    "    unique_classes = data[class_column].unique()\n",
    "\n",
    "    # Create a color map for plotting different classes\n",
    "    if colors is None:\n",
    "        colors = {class_label: f'rgb({i * 10}, {255 - i * 40}, {i * 20})'\n",
    "                  for i, class_label in enumerate(unique_classes)}\n",
    "\n",
    "    for class_label in unique_classes:\n",
    "        class_data = data[data[class_column] == class_label].drop(class_column, axis=1)\n",
    "        mean_spectrum = class_data.mean()\n",
    "\n",
    "        # Add a trace for the mean spectrum of each class\n",
    "        fig.add_trace(go.Scatter(x=mean_spectrum.index, y=mean_spectrum.values,\n",
    "                                 mode='lines', name=f'Class {class_label}',\n",
    "                                 line=dict(color=colors.get(class_label, 'blue'))))\n",
    "\n",
    "        if threshold:\n",
    "            # Highlight peaks exceeding the threshold on the y-axis\n",
    "            max_value = mean_spectrum.max()\n",
    "            for mz, intensity in zip(mean_spectrum.index, mean_spectrum.values):\n",
    "                if intensity > threshold:\n",
    "                    fig.add_annotation(\n",
    "                        go.layout.Annotation(\n",
    "                            x=mz,\n",
    "                            y=intensity,\n",
    "                            text=mz,\n",
    "                            showarrow=True,\n",
    "                            arrowhead=3,\n",
    "                            arrowsize=1,\n",
    "                            arrowwidth=2,\n",
    "                            arrowcolor='green',\n",
    "                            ax=0,\n",
    "                            ay=-30,\n",
    "                            yref='y',\n",
    "                            xref='x'\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    # Update layout and axis titles\n",
    "    fig.update_layout(width=1000, xaxis_title='m/z', yaxis_title='Relative Intensities')\n",
    "    fig.update_xaxes(tickangle=45, tickfont=dict(size=10))  # Adjust the font size of x-axis ticks\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "    preprocess data.\n",
    "    Args:\n",
    "        pd.DataFrame: data.\n",
    "    Returns:\n",
    "        pd.DataFrame: Features (X).\n",
    "        pd.Series: Target (y).\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "\n",
    "    train = pd.read_csv(data_file)\n",
    "\n",
    "    # Data preprocessing\n",
    "    train_id = train\n",
    "    train = train.drop([\"File\", \"Sum.\", \"Start scan\", \"End scan\"], axis=1) # adapt this to your specific columns\n",
    "\n",
    "    # Split data into features (X) and target (y)\n",
    "    y = train.pop('Class')\n",
    "    X = train\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def lazy_predict(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform lazy prediction of multiple models.\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_test (pd.Series): Testing target.\n",
    "    Returns:\n",
    "        pd.DataFrame: LazyClassifier results.\n",
    "        pd.DataFrame: Model predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize LazyClassifier\n",
    "    clf = LazyClassifier(verbose=0, predictions=True)\n",
    "    models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Print the models\n",
    "    print(models)\n",
    "\n",
    "    return models, predictions\n",
    "\n",
    "def find_and_build_best_model(models, X_train, y_train, specific_model=None):\n",
    "    \"\"\"\n",
    "    Find and build the best model based on Lazy Predict results or a specific model if specified.\n",
    "    Args:\n",
    "        models (pd.DataFrame): LazyClassifier results.\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        specific_model (str, optional): If provided, build the specified model (e.g., 'RidgeClassifier').\n",
    "    Returns:\n",
    "        str: Best model name.\n",
    "        Pipeline: Best model pipeline.\n",
    "    \"\"\"\n",
    "    best_model_name = None\n",
    "    best_f1_score = -1\n",
    "\n",
    "    # If a specific model is specified, use it directly\n",
    "    if specific_model:\n",
    "        best_model_name = specific_model\n",
    "    else:\n",
    "        # Find the best model based on Lazy Predict results\n",
    "        for model_name in models.index:\n",
    "            f1_score = models.at[model_name, 'F1 Score']\n",
    "            if f1_score > best_f1_score:\n",
    "                best_f1_score = f1_score\n",
    "                best_model_name = model_name\n",
    "\n",
    "    if best_model_name:\n",
    "        print(\"Best Classifier:\", best_model_name)\n",
    "\n",
    "        try:\n",
    "            # If the specific model is RidgeClassifier, use it directly\n",
    "            if best_model_name == 'RidgeClassifier':\n",
    "                best_model = RidgeClassifier()\n",
    "            else:\n",
    "                model_module = importlib.import_module('sklearn.linear_model')\n",
    "                if hasattr(model_module, best_model_name):\n",
    "                    best_model = getattr(model_module, best_model_name)()\n",
    "                else:\n",
    "                    model_module = importlib.import_module('sklearn.ensemble')\n",
    "                    if hasattr(model_module, best_model_name):\n",
    "                        best_model = getattr(model_module, best_model_name)()\n",
    "                    else:\n",
    "                        model_module = importlib.import_module('sklearn.svm')\n",
    "                        if hasattr(model_module, best_model_name):\n",
    "                            best_model = getattr(model_module, best_model_name)()\n",
    "                        else:\n",
    "                            # Check if it's a LightGBM classifier\n",
    "                            if best_model_name.startswith(\"LGBM\"):\n",
    "                                best_model = getattr(lgb, best_model_name)()\n",
    "                            # Check if it's an XGBoost classifier\n",
    "                            elif best_model_name.startswith(\"XGB\"):\n",
    "                                best_model = getattr(xgb, best_model_name)()\n",
    "                            else:\n",
    "                                print(\"Best Classifier not found.\")\n",
    "                                return None, None\n",
    "            \n",
    "            pipeline = Pipeline([('scaler', StandardScaler()), (best_model_name, best_model)])\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            return best_model_name, pipeline\n",
    "        except ImportError:\n",
    "            print(\"Best Classifier not found.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Best Classifier not found.\")\n",
    "        return None, None\n",
    "\n",
    "def confusion_matrix_scores_classification_report(pipeline, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Show scores, confusion matrix, and classification report.\n",
    "    Args:\n",
    "        pipeline (Pipeline): Trained model pipeline.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_test (pd.Series): Testing target.\n",
    "    \"\"\"\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print('Accuracy:', score)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    ConfusionMatrixDisplay.from_estimator(pipeline, X_test, y_test)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 15)\n",
    "    plt.show()\n",
    "\n",
    "def cross_validate_and_report(pipeline, X, y):\n",
    "    \"\"\"\n",
    "    K-fold cross-validation and show scores, classification report, and confusion matrix.\n",
    "    Args:\n",
    "        pipeline (Pipeline): Trained model pipeline.\n",
    "        X (pd.DataFrame): Features.\n",
    "        y (pd.Series): Target.\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "    print('CV Scores:', cv_scores)\n",
    "    print('Mean CV Score:', cv_scores.mean())\n",
    "    print('Std CV Score:', cv_scores.std())\n",
    "\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=kfold)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    class_names = pipeline.named_steps[pipeline.steps[-1][0]].classes_\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='viridis')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=class_names, yticklabels=class_names, title='Confusion matrix', ylabel='True label', xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"black\" if cm[i, j] > thresh else \"yellow\")\n",
    "    fig.tight_layout()\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 15)\n",
    "    plt.show()\n",
    "\n",
    "def eli5_feature_importance(pipeline, X_train, top_features=40):\n",
    "    \"\"\"\n",
    "    Show feature importances using Eli5 (LIME (contribution) or permutation(weights).\n",
    "    Args:\n",
    "        pipeline (Pipeline): Trained model pipeline.\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        top_features (int): Number of top features to display.\n",
    "    \"\"\"\n",
    "    model = pipeline.named_steps[pipeline.steps[-1][0]]\n",
    "    sample_contribution = eli5.show_weights(model, feature_names=X_train.columns.tolist(), top=top_features, feature_re='^.*$')\n",
    "    return sample_contribution\n",
    "\n",
    "def save_contributions(csv_name, pipeline, X_train):\n",
    "    \"\"\"\n",
    "    Save contributions in CSV format for all samples in the train set.\n",
    "    \"\"\"\n",
    "    model = pipeline.named_steps[pipeline.steps[-1][0]]\n",
    "    sample_contributions = []\n",
    "\n",
    "    for idx in range(len(X_train.index)):\n",
    "        sample_contribution_df = eli5.explain_weights_df(model, feature_names=X_train.columns.tolist(), feature_re='^.*$')\n",
    "        sample_contributions.append(sample_contribution_df)\n",
    "\n",
    "    all_contributions_df = pd.concat(sample_contributions)\n",
    "    all_contributions_df.to_csv(csv_name, index=False)\n",
    "    \n",
    "\n",
    "def peak_picking(ms_data, min_sn=10):\n",
    "    \"\"\"\n",
    "    Performs peak picking on mass spectrometry data using S/N ratio and find_peaks.\n",
    "    Args:\n",
    "        ms_data (pd.DataFrame): Mass spectrometry data.\n",
    "        min_sn (int): Minimum signal-to-noise ratio for peak picking.\n",
    "    Returns:\n",
    "        pd.DataFrame: Picked peaks data.\n",
    "    \"\"\"\n",
    "    peaks_df_list = []\n",
    "    ms_data = data.drop([\"Class\", \"Start scan\", \"End scan\", \"Sum.\", \"File\"], axis=1)\n",
    "    for i in range(len(ms_data)):\n",
    "        spectrum = ms_data.iloc[i].values\n",
    "\n",
    "        # Calculate the noise level as the standard deviation of the spectrum\n",
    "        noise_std = np.std(spectrum)\n",
    "\n",
    "        # Calculate the threshold based on the noise level and min_sn\n",
    "        threshold = noise_std * min_sn\n",
    "\n",
    "        # Find peaks using the threshold\n",
    "        peaks, _ = find_peaks(spectrum, height=threshold)\n",
    "\n",
    "        peaks_df_i = pd.DataFrame({\n",
    "            'spectrum_index': i,\n",
    "            'm/z': ms_data.columns[peaks],\n",
    "            'intensity': spectrum[peaks],\n",
    "        })\n",
    "\n",
    "        peaks_df_list.append(peaks_df_i)\n",
    "\n",
    "    peaks_df = pd.concat(peaks_df_list, ignore_index=True)\n",
    "    peaks_df = peaks_df.dropna(subset=['m/z'])\n",
    "    peaks_df = peaks_df.pivot_table(index='spectrum_index', columns='m/z', values='intensity')\n",
    "    data_pick_picked = pd.concat([peaks_df,data['Class']], axis=1)\n",
    "    data_pick_picked = data_pick_picked.fillna(0)\n",
    "    data_pick_picked\n",
    "    \n",
    "    return data_pick_picked \n",
    "\n",
    "def create_heatmap(data,cmap='viridis_r', distance_metric='cosine', z_score=0):\n",
    "    \"\"\"\n",
    "    Create a heatmap with custom colors for over-expression (red) and under-expression (green) with black for no correlation.\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data for creating the heatmap.\n",
    "        distance_metric (str): Distance metric for clustering (e.g., 'cosine', 'euclidean').\n",
    "        z_score (int): Z-score for data normalization.\n",
    "    \"\"\"\n",
    "    # Create the heatmap using the custom colormap\n",
    "    sns.clustermap(data.groupby('Class').mean().T, cmap=cmap, center=0, col_cluster=False, row_cluster=True, metric=distance_metric, z_score=z_score, cbar_kws={'label': ''}, cbar=True, xticklabels=True, yticklabels=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def significant_features(data, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Identify significant features using the Kruskal-Wallis test.\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data containing features and target.\n",
    "        alpha (float): Significance level for the test.\n",
    "    Returns:\n",
    "        list: Significant feature column names.\n",
    "    \"\"\"\n",
    "    x = 'Class'\n",
    "    y_columns = data.columns.tolist()\n",
    "    \n",
    "    # Check if 'Class' is in the list of columns\n",
    "    if x in y_columns:\n",
    "        y_columns.remove(x)\n",
    "    else:\n",
    "        print(\"'Class' column not found in the data.\")\n",
    "        return None\n",
    "\n",
    "    order = data[x].unique()\n",
    "    significant_columns = []\n",
    "\n",
    "    num_comparisons = len(y_columns)  # Number of comparisons (number of features)\n",
    "\n",
    "    # Apply Bonferroni correction\n",
    "    corrected_alpha = alpha / num_comparisons\n",
    "\n",
    "    for col in y_columns:\n",
    "        data_dict = {group: data[col][data[x] == group] for group in order}\n",
    "        test_statistic, p_value = kruskal(*data_dict.values())\n",
    "        \n",
    "        if p_value <= corrected_alpha:\n",
    "            significant_columns.append(col)\n",
    "\n",
    "    return significant_columns\n",
    "\n",
    "\n",
    "def boxplot_significant_features(data, mz_values, class_colors=None, test='Kruskal', loc='inside'):\n",
    "    \"\"\"\n",
    "    Create box plots for significant features.\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data containing features and target.\n",
    "        significant_features (list): List of significant feature column names.\n",
    "        class_colors (dict): Custom class colors.\n",
    "        test (str): Statistical test to use ('Kruskal' by default).\n",
    "    \"\"\"\n",
    "    # Define order and box pairs\n",
    "    label = 'Class'\n",
    "    order = sorted(data[label].unique())  # Sort the unique class labels\n",
    "    # Generate box pairs using itertools.combinations\n",
    "    box_pairs = list(combinations(order, 2))\n",
    "    print(\"Class labels in dataset:\", order)  # Add this line for debugging\n",
    "    \n",
    "    # Create a custom color palette mapping class labels to colors\n",
    "    custom_palette = {class_label: class_colors.get(class_label, 'blue') for class_label in order}\n",
    "    \n",
    "    # Calculate the number of rows and columns for the grid layout\n",
    "    num_mz_values = len(mz_values)\n",
    "    num_cols = int(num_mz_values ** 0.5)  # Calculate the number of columns based on sqrt(num_mz_values)\n",
    "    num_rows = (num_mz_values + num_cols - 1) // num_cols  # Calculate the number of rows\n",
    "    \n",
    "    # Calculate the figure size based on the number of box plots\n",
    "    figsize_x = 16\n",
    "    figsize_y = 5 * num_rows\n",
    "    \n",
    "    # Create a figure and axis grid for the boxplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(figsize_x, figsize_y), dpi=100)\n",
    "    \n",
    "    # Flatten the axes array if there's only one row\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, mz in enumerate(mz_values):\n",
    "        x = \"Class\"\n",
    "        y = mz\n",
    "\n",
    "        # Select the current axis\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Create a boxplot on the current axis with custom class color\n",
    "        sns.boxplot(data=data, x=x, y=y, order=order, ax=ax, palette=custom_palette)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        \n",
    "        # Add simplified statistical annotations to the boxplot\n",
    "        add_stat_annotation(ax, data=data, x=x, y=y, order=order, box_pairs=box_pairs,\n",
    "                            test=test, text_format='star', loc='inside', verbose=0)\n",
    "        \n",
    "        # Set smaller font size for y-axis labels\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(num_mz_values, num_rows * num_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "    \n",
    "    # Adjust layout and spacing of subplots\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the final figure\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "        \n",
    "def violin_significant_features(data, mz_values, class_colors=None, test='Kruskal', loc='inside'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create violin plots for significant features.\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data containing features and target.\n",
    "        significant_features (list): List of significant feature column names.\n",
    "        class_colors (dict): Custom class colors.\n",
    "        test (str): Statistical test to use ('Kruskal' by default).\n",
    "    \"\"\"\n",
    "    label = 'Class'\n",
    "    order = sorted(data[label].unique())  # Sort the unique class labels\n",
    "    box_pairs = list(combinations(order, 2))\n",
    "\n",
    "    print(\"Class labels in dataset:\", order)  # Add this line for debugging\n",
    "\n",
    "    # Create a custom color palette mapping class labels to colors\n",
    "    custom_palette = {class_label: class_colors.get(class_label, 'blue') for class_label in order}\n",
    "\n",
    "    # Calculate the number of rows and columns for the grid layout\n",
    "    num_mz_values = len(mz_values)\n",
    "    num_cols = int(num_mz_values ** 0.5)  # Calculate the number of columns based on sqrt(num_mz_values)\n",
    "    num_rows = (num_mz_values + num_cols - 1) // num_cols  # Calculate the number of rows\n",
    "\n",
    "    # Calculate the figure size based on the number of violin plots\n",
    "    figsize_x = 16\n",
    "    figsize_y = 5 * num_rows\n",
    "\n",
    "    # Create a figure and axis grid for the violin plots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(figsize_x, figsize_y), dpi=100)\n",
    "\n",
    "    # Flatten the axes array if there's only one row\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, mz in enumerate(mz_values):\n",
    "        x = \"Class\"\n",
    "        y = mz\n",
    "\n",
    "        # Select the current axis\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Create a violin plot on the current axis with custom class color\n",
    "        sns.violinplot(data=data, x=x, y=y, order=order, inner=\"quart\", ax=ax, palette=custom_palette)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "        # Add simplified statistical annotations to the violin plot\n",
    "        add_stat_annotation(ax, data=data, x=x, y=y, order=order, box_pairs=box_pairs,\n",
    "                            test=test, text_format='star', loc='inside', verbose=0)\n",
    "\n",
    "        # Set smaller font size for y-axis labels\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "    # Remove empty subplots\n",
    "    for i in range(num_mz_values, num_rows * num_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    # Adjust layout and spacing of subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the final figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb3c45",
   "metadata": {},
   "source": [
    "# Usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d07be",
   "metadata": {},
   "source": [
    "# 1- Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "data_file ='data.csv'\n",
    "\n",
    "# Import data\n",
    "data = import_data(data_file)\n",
    "print(\"Labels :\",data.Class.unique())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b49e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display spectra \n",
    "custom_colors = {'Class1': 'green', 'Class2': 'pink', 'Class3'}  # Customize class colors\n",
    "plot = plot_mean_spectra(data, class_column='Class', threshold=None, colors=custom_colors) \n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models\n",
    "X, y =process_data(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)\n",
    "models, predictions = lazy_predict(X_train, y_train, X_test, y_test)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model or a specific model if specified\n",
    "best_model_name, best_model_pipeline = find_and_build_best_model(models, X_train, y_train, specific_model=None) # specific_model='RidgeClassifierCV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7704d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the best model for 20% out validation \n",
    "confusion_matrix_scores_classification_report(best_model_pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ab4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the best model with 5-fold cv\n",
    "cross_validate_and_report(best_model_pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6dcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model for blind test\n",
    "joblib.dump(best_model_pipeline, \"Model_X.pkl\") # put your model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature contributions\n",
    "sample_contribution = eli5_feature_importance(best_model_pipeline, X_train)\n",
    "sample_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_contributions(\"LIME_X.csv\", best_model_pipeline, X_train)  # put you csv name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526fcf7",
   "metadata": {},
   "source": [
    "# 2-Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak picking \n",
    "data_peak_picked = peak_picking(data, min_sn=10) # specify the S/N \n",
    "data_peak_picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap\n",
    "create_heatmap(data_peak_picked,cmap='viridis_r',distance_metric='cosine', z_score=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb945823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significant features\n",
    "significant_mz_values = significant_features(data_peak_picked)\n",
    "print(len(significant_mz_values))\n",
    "significant_mz_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for significant features\n",
    "boxplot_significant_features(data, significant_mz_values1, class_colors=custom_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create violin plots for significant features\n",
    "violin_significant_features(data, significant_mz_values, class_colors=custom_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
